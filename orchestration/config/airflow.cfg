[core]
# The home folder for airflow, default is ~/airflow
airflow_home = /opt/airflow

# The folder where your airflow DAGs are stored
dags_folder = /opt/airflow/dags

# The folder where airflow should store its log files
base_log_folder = /opt/airflow/logs

# Airflow can store logs remotely in AWS S3, Google Cloud Storage or Elastic Search
remote_logging = False

# The executor class that airflow should use
executor = LocalExecutor

# The SqlAlchemy connection string to the metadata database
sql_alchemy_conn = postgresql+psycopg2://airflow:airflow@postgres/airflow

# The amount of parallelism as a setting to the executor
parallelism = 32

# The number of task instances allowed to run concurrently by the scheduler
dag_concurrency = 16

# Are DAGs paused by default at creation
dags_are_paused_at_creation = True

# The maximum number of active DAG runs per DAG
max_active_runs_per_dag = 16

# Whether to load the DAG examples that ship with Airflow
load_examples = False

# The default user defined in the Admin section
default_user = admin

# Secret key to save connection passwords in the db
fernet_key = 81HqDtbqAywKSOumSha3BhWNOdQ26slT6K0YaZeZyPs=

# Whether to disable pickling dags
donot_pickle = False

# How long before timing out a python file import
dagbag_import_timeout = 30

# How long before timing out a DagFileProcessor, which processes a dag file
dag_file_processor_timeout = 50

# The class to use for running task instances in a subprocess
task_runner = StandardTaskRunner

# If set, tasks without a `run_as_user` argument will be run with this user
default_impersonation = 

# What security module to use
security = 

# Turn unit test mode on (overwrites many configuration options with test values)
unit_test_mode = False

# Whether to enable pickling for xcom (note that this is insecure)
enable_xcom_pickling = False

# When a task is killed, this defines the default killed task exit code
killed_task_cleanup_time = 60

# The scheduler constantly tries to trigger new tasks
scheduler_heartbeat_sec = 5

# Task instances listen for external kill signal
task_log_reader = task

# The number of times to try to schedule each DAG file
max_threads = 2

# Should tasks be authenticated
authenticate = False

# Secret key used to run your flask app
secret_key = a25mQ1FHTUh3MnFRSk5KMEIyVVU2YmN0VGRyYTVXY08=

[webserver]
# The base url of your website as airflow cannot guess what domain or cname you are using
base_url = http://localhost:8080

# Default timezone to display all dates in the UI
default_ui_timezone = UTC

# The port on which to run the web server
web_server_port = 8080

# Paths to the SSL certificate and key for the web server
web_server_ssl_cert = 
web_server_ssl_key = 

# Number of seconds the webserver waits before killing gunicorn master that doesn't respond
web_server_master_timeout = 120

# Number of seconds the gunicorn webserver waits before timing out on a worker
web_server_worker_timeout = 120

# Number of workers to refresh at a time
worker_refresh_batch_size = 1

# Number of workers to refresh at a time
worker_refresh_interval = 30

# Secret key used to run your flask app
secret_key = a25mQ1FHTUh3MnFRSk5KMEIyVVU2YmN0VGRyYTVXY08=

# Number of workers to run the webserver on
workers = 4

# The worker class gunicorn should use
worker_class = sync

# Log files for the gunicorn webserver
access_logfile = -
error_logfile = -

# Expose the configuration file in the web server
expose_config = False

# Set to true to turn on authentication
authenticate = False

# Filter the list of dags by owner name
filter_by_owner = False

# Default DAG view
dag_default_view = graph

# Default DAG orientation
dag_orientation = LR

# Puts the webserver in demonstration mode
demo_mode = False

# The amount of time (in secs) webserver will wait for initial handshake
web_server_worker_timeout = 120

# Set to true if you want to use the cookie authentication
cookie_secure = False

# Set to true if you want to use the cookie authentication with SSL
cookie_samesite = Lax

# Default timezone to display all dates in the UI
default_ui_timezone = UTC

# Enable werkzeug
enable_proxy_fix = False

# Set to true if you want to use the cookie authentication
cookie_secure = False

[scheduler]
# Task instances listen for external kill signal
job_heartbeat_sec = 5

# The scheduler constantly tries to trigger new tasks
scheduler_heartbeat_sec = 5

# Number of seconds after which a DAG file is parsed
run_duration = -1

# The number of times to try to schedule each DAG file
min_file_process_interval = 0

# Number of seconds after which a DAG can be triggered
dag_dir_list_interval = 300

# How often should stats be printed to the logs
print_stats_interval = 30

# If the last scheduling loop for a DAG took more than this number of seconds
scheduler_health_check_threshold = 30

# Number of seconds after which a DAG file is parsed
child_process_timeout = 60

# The number of times to retry a failed task
max_tis_per_query = 512

# Should the scheduler scan for DAGs in subdirectories
statsd_on = False

# Hostname of the statsd server
statsd_host = localhost

# Port of the statsd server
statsd_port = 8125

# Prefix to use for all stats
statsd_prefix = airflow

# Allow the scheduler to continue running even if a DAG file is not parseable
catchup_by_default = False

# Number of seconds after which a DAG file is parsed
max_dagruns_to_create_per_loop = 10

# Number of seconds after which a DAG file is parsed
max_dagruns_per_loop_to_schedule = 20

# Turn off scheduler use of cron intervals by setting this to False
use_job_schedule = True

# Allow the scheduler to continue running even if a DAG file is not parseable
allow_trigger_in_future = False

[smtp]
# Configuration for sending email alerts
smtp_host = localhost
smtp_starttls = True
smtp_ssl = False
smtp_port = 587
smtp_mail_from = airflow@animeverse.com
smtp_user = 
smtp_password = 

[logging]
# The folder where airflow should store its log files
base_log_folder = /opt/airflow/logs

# Logging level
logging_level = INFO

# Logging class
logging_config_class = 

# Log format
log_format = [%%(asctime)s] {%%(filename)s:%%(lineno)d} %%(levelname)s - %%(message)s

# Log format for colored logs
colored_log_format = [%%(blue)s%%(asctime)s%%(reset)s] {%%(blue)s%%(filename)s:%%(reset)s%%(lineno)d} %%(log_color)s%%(levelname)s%%(reset)s - %%(log_color)s%%(message)s%%(reset)s

# Log format for remote logging
remote_log_conn_id = 

# Remote base log folder
remote_base_log_folder = 

# Encrypt logs in S3
encrypt_s3_logs = False