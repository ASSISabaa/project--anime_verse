[core]
# The home folder for airflow, default is ~/airflow
airflow_home = /opt/airflow

# The folder where your airflow pipelines live
dags_folder = /opt/airflow/dags

# The folder where airflow plugins are stored
plugins_folder = /opt/airflow/plugins

# The base log folder
base_log_folder = /opt/airflow/logs

# Logging configuration
remote_logging = False

# Executor to use
executor = LocalExecutor

# The SqlAlchemy connection string to the metadata database
sql_alchemy_conn = postgresql+psycopg2://airflow:airflow@postgres/airflow

# The amount of parallelism as a setting to the executor
parallelism = 32

# The number of task instances allowed to run concurrently by the scheduler
dag_concurrency = 16

# Are DAGs paused by default at creation
dags_are_paused_at_creation = True

# The maximum number of active DAG runs per DAG
max_active_runs_per_dag = 16

# Whether to load the examples that ship with Airflow
load_examples = False

# Path to custom plugins
plugins_folder = /opt/airflow/plugins

# Secret key to save connection passwords in the db
fernet_key = 

# Whether to disable pickling dags
donot_pickle = False

# How long before timing out a python file import
dagbag_import_timeout = 30

# How long before timing out a DagFileProcessor
dag_file_processor_timeout = 50

# The class to use for running task instances in a subprocess
task_runner = StandardTaskRunner

# If set, tasks without a `run_as_user` argument will be run with this user
default_impersonation = 

# Security section
security = 

# Turn unit test mode on (overwrites many configuration options with test values)
unit_test_mode = False

# Whether to enable pickling for xcom
enable_xcom_pickling = True

[logging]
# The folder where airflow should store its log files
base_log_folder = /opt/airflow/logs

# Airflow logging level
logging_level = INFO

# Logging formatter
log_format = [%%(asctime)s] {%%(filename)s:%%(lineno)d} %%(levelname)s - %%(message)s
simple_log_format = %%(asctime)s %%(levelname)s - %%(message)s

[webserver]
# The base url of your website as airflow cannot guess what domain or cname you are using
base_url = http://localhost:8082

# Default timezone to display all dates in the UI
default_ui_timezone = UTC

# The port on which to run the web server
web_server_port = 8080

# Paths to the SSL certificate and key for the web server
web_server_ssl_cert = 
web_server_ssl_key = 

# Number of workers to run the webserver on
workers = 4

# The worker timeout for the webserver
worker_timeout = 120

# The hostname used to access the webserver
hostname = 0.0.0.0

# Expose the configuration file in the web server
expose_config = True

# Default DAG view
dag_default_view = tree

# Default DAG orientation
dag_orientation = LR

# Filter the list of dags by owner name
filter_by_owner = False

# Amount of time (in secs) webserver will wait for initial handshake
worker_refresh_interval = 30

# Amount of time (in secs) webserver will wait before killing gunicorn master that doesn't respond
worker_refresh_batch_size = 1

# Secret key used to run your flask app
secret_key = temporary_key

# In what way should the web server authenticate users
authenticate = False

# The authentication backend to use
auth_backend = airflow.api.auth.backend.default

[scheduler]
# Task instances listen for external kill signal
job_heartbeat_sec = 5

# The scheduler constantly tries to trigger new tasks
scheduler_heartbeat_sec = 5

# Number of times the scheduler should run before exiting
num_runs = -1

# The number of times to try to schedule each DAG file
processor_poll_interval = 1

# How often (in seconds) to scan the DAGs directory for new files
min_file_process_interval = 0

# How often should stats be printed to the logs
print_stats_interval = 30

# If True, dag file will be checked for new tasks
dag_dir_list_interval = 300

# How often should the scheduler check for orphaned tasks and SchedulerJobs
orphaned_tasks_check_interval = 300.0

# How often should the scheduler check for zombies
zombie_task_threshold = 300

# How often should the scheduler perform a cleanup of old DAG runs and task instances
schedule_after_task_execution = True

# Turn off scheduler catchup by setting this to False
catchup_by_default = True

# Number of task slots per executor 
max_threads = 2

# Enable health check
enable_health_check = True

[operators]
# The default owner assigned to each new operator
default_owner = airflow

[smtp]
# SMTP configuration
smtp_host = localhost
smtp_starttls = True
smtp_ssl = False
smtp_port = 587
smtp_mail_from = airflow@localhost

[celery]
# This section only applies if you are using the CeleryExecutor in
# [core] section above
celery_app_name = airflow.executors.celery_executor

# The Celery broker URL
broker_url = redis://redis:6379/0

# The Celery result backend
result_backend = db+postgresql://airflow:airflow@postgres/airflow

# Celery configuration options
worker_concurrency = 16

[api]
# How to authenticate users of the API
auth_backend = airflow.api.auth.backend.default

[admin]
# UI to hide sensitive variable fields when set to True
hide_sensitive_variable_fields = True